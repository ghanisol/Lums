// Assignment 3: Web crawler
// Deadline: Saturday 18 March at 9PM
//
// You will write a web crawler in this assignment. Make sure to not 
// send out too many requests. It is best to try on local sites and with
// a small limit on the number of requests. 
// 
// You will read the configuration from a file named 'config.json' in the 
// same directory. Here is a sample config.json. Use promises to read this
// file and JSON.parse to parse it. Do not add comments in config.json
{
    "maxRequestsPerSite": 5,  // do not send more than this many requests 
                              // to a single website
    "minFetchInterval": 1000, // your requests to a particular website must 
                              // be at least this many milliseconds apart
    "maxRequests": 100,       // stop after sending this many requests
    "initialUrls": [            
        "http://rootdomain.com/thispage",
        "https://thisdomain.com/thatpage"
    ]
}

// Maintain a map of how many requests you have made to each site and the 
// last promised Delay like this. This is just an example. You will not
// hardcode values. Every time you make a request you will use a promise 
// that resolves when the response ends and you chain another promise to
// it that will resolve after a set amount of time and store it in this
// map so further requests can be chained to this promised delay.
siteInfo = {
    'rootDomain.com': {
        requestCount: 2,
        promisedDelay: ...
    },
    'anotherDomain.com': {
        requestCount: 1,
        promisedDelay: ...
    }
}
// For each of the URLs in the initialURLs and afterwards when you find 
// new URLs, you will separate the domain name, and if it is less than max 
// requests for that site, attach the next request to the then handler of 
// promisedDelay and also attach a delay promise. Use the following delay 
// function that returns a promise
const delay = msecs => new Promise(resolve => setTimeout(resolve, msecs))

// For fetching, write a promise based fetch function that uses http.get 
// or https.get based on the URL. Once you get the response you will pipe 
// it to parse5 (http://inikulin.github.io/parse5/classes/saxparser.html) 
// and in the startTag handler of parser, you find if it is an "A" tag. If 
// so, you take the "href" attribute. This is an outgoing URL. If it starts 
// with "//" it means use http:// or https:// based on the current page's 
// URL. If it starts with any alphabets followed by :// you discard it 
// except when it is http or https. Finally, if it starts with "/" it is an 
// absolute URL and it means to attach the whole http://domainname before 
// it and in other cases, it is a relative URL and you attach 
// http://domainname/currentpage/ before it.
// 
// At the end, you have to print a list of all domains encountered 
// alongwith the number of distinct domains that have a page with a link to 
// a page on this domain (incoming links from distinct domains) and the 
// number of distint domains linked to by this domain (outgoing links to 
// distinct domains)
//
// Here is one way to divide your work.
//
// Step 1: Read and parse config.json
// Step 2: Download URLs listed in config.json
// Step 3: Obey delay and limit per site and max URLs to fetch
// Step 4: Extract outgoing links from webpages and add to request queue
// Step 5: Maintain and print list of domains with counts

